{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bMd-Vbw7fVk",
        "outputId": "32216814-bc5f-4774-841a-a47dc6efc95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import torch\n",
        "\n",
        "class Config:\n",
        "    TRAIN_PATH = '/content/small-train.json'\n",
        "    DEV_PATH = '/content/small-dev.json'\n",
        "    TEST_PATH = '/content/small-test.json'\n",
        "\n",
        "    # JSON Keys\n",
        "    SRC_KEY = 'english'\n",
        "    TGT_KEY = 'vietnamese'\n",
        "\n",
        "    # Model Params (Ch·ªânh s·ª≠a tho·∫£i m√°i t·∫°i ƒë√¢y)\n",
        "    D_MODEL = 256\n",
        "    N_ENC_LAYERS = 3\n",
        "    N_DEC_LAYERS = 3\n",
        "    DROPOUT = 0.5\n",
        "\n",
        "    # Training Params\n",
        "    BATCH_SIZE = 64\n",
        "    LEARNING_RATE = 0.001\n",
        "    NUM_EPOCHS = 15 # TƒÉng l√™n v√¨ ch·∫°y GPU nhanh h∆°n\n",
        "    CLIP = 1.0\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Special Tokens\n",
        "    PAD_IDX = 0\n",
        "    BOS_IDX = 1\n",
        "    EOS_IDX = 2\n",
        "    UNK_IDX = 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vocab.py\n",
        "import json\n",
        "from collections import Counter\n",
        "from config import Config\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.stoi = {\n",
        "            \"<pad>\": Config.PAD_IDX,\n",
        "            \"<bos>\": Config.BOS_IDX,\n",
        "            \"<eos>\": Config.EOS_IDX,\n",
        "            \"<unk>\": Config.UNK_IDX,\n",
        "        }\n",
        "        self.itos = {v: k for k, v in self.stoi.items()}\n",
        "        self.total_src_tokens = 4\n",
        "        self.total_tgt_tokens = 4\n",
        "\n",
        "    def build_vocab(self, json_path, src_key, tgt_key, min_freq=2):\n",
        "        print(f\"ƒêang x√¢y d·ª±ng Vocab t·ª´ {json_path}...\")\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        src_counter = Counter()\n",
        "        tgt_counter = Counter()\n",
        "\n",
        "        for item in data:\n",
        "            # Tokenize ƒë∆°n gi·∫£n b·∫±ng split(), c√≥ th·ªÉ d√πng spacy n·∫øu mu·ªën x·ªãn h∆°n\n",
        "            src_tokens = item[src_key].lower().split()\n",
        "            tgt_tokens = item[tgt_key].lower().split()\n",
        "\n",
        "            src_counter.update(src_tokens)\n",
        "            tgt_counter.update(tgt_tokens)\n",
        "\n",
        "        # Add Source Tokens\n",
        "        for word, freq in src_counter.items():\n",
        "            if freq >= min_freq and word not in self.stoi:\n",
        "                self.stoi[word] = len(self.stoi)\n",
        "\n",
        "        self.total_src_tokens = len(self.stoi)\n",
        "\n",
        "        # Add Target Tokens (L∆∞u √Ω: Th∆∞·ªùng ng∆∞·ªùi ta t√°ch 2 vocab ri√™ng,\n",
        "        # nh∆∞ng ƒë·ªÉ ƒë∆°n gi·∫£n cho lab n√†y ta g·ªôp chung ho·∫∑c ch·ªâ add th√™m t·ª´ m·ªõi)\n",
        "        for word, freq in tgt_counter.items():\n",
        "            if freq >= min_freq and word not in self.stoi:\n",
        "                self.stoi[word] = len(self.stoi)\n",
        "\n",
        "        self.total_tgt_tokens = len(self.stoi) # T·ªïng size vocab chung\n",
        "        self.itos = {v: k for k, v in self.stoi.items()}\n",
        "        print(f\"Vocab size: {len(self.stoi)}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Text string -> List of Indices\n",
        "        tokens = text.lower().split()\n",
        "        return [Config.BOS_IDX] + [self.stoi.get(token, Config.UNK_IDX) for token in tokens] + [Config.EOS_IDX]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        # List of Indices -> Text String\n",
        "        tokens = []\n",
        "        for idx in indices:\n",
        "            if idx == Config.EOS_IDX: break\n",
        "            if idx in [Config.BOS_IDX, Config.PAD_IDX]: continue\n",
        "            tokens.append(self.itos.get(idx, \"<unk>\"))\n",
        "        return \" \".join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX2VY37CAPz-",
        "outputId": "84043b7f-5778-480a-d34b-c063f1adeb5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vocab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from config import Config\n",
        "\n",
        "# ==========================================\n",
        "# 1. VANILLA SEQ2SEQ\n",
        "# ==========================================\n",
        "class VanillaSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.src_embedding = nn.Embedding(vocab.total_tgt_tokens, Config.D_MODEL, padding_idx=Config.PAD_IDX)\n",
        "        self.tgt_embedding = nn.Embedding(vocab.total_tgt_tokens, 2*Config.D_MODEL, padding_idx=Config.PAD_IDX)\n",
        "\n",
        "        self.encoder = nn.LSTM(Config.D_MODEL, Config.D_MODEL, Config.N_ENC_LAYERS,\n",
        "                               batch_first=True, dropout=Config.DROPOUT, bidirectional=True)\n",
        "\n",
        "        self.decoder = nn.LSTM(2*Config.D_MODEL, 2*Config.D_MODEL, Config.N_DEC_LAYERS,\n",
        "                               batch_first=True, dropout=Config.DROPOUT, bidirectional=False)\n",
        "\n",
        "        self.output_head = nn.Linear(2*Config.D_MODEL, vocab.total_tgt_tokens)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        embedded_x = self.src_embedding(x)\n",
        "        _, (hidden, cell) = self.encoder(embedded_x)\n",
        "\n",
        "        # Merge bidirectional hidden states\n",
        "        hidden = hidden.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "        cell = cell.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "\n",
        "        dec_input = y[:, 0].unsqueeze(1)\n",
        "        dec_hidden, dec_cell = hidden, cell\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(1, y.shape[1]):\n",
        "            embedded_input = self.tgt_embedding(dec_input)\n",
        "            output, (dec_hidden, dec_cell) = self.decoder(embedded_input, (dec_hidden, dec_cell))\n",
        "            prediction = self.output_head(output.squeeze(1))\n",
        "            outputs.append(prediction.unsqueeze(1))\n",
        "            dec_input = y[:, t].unsqueeze(1) # Teacher forcing\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "    def predict(self, x):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            embedded_x = self.src_embedding(x)\n",
        "            _, (hidden, cell) = self.encoder(embedded_x)\n",
        "            hidden = hidden.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "            cell = cell.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "\n",
        "            dec_input = torch.full((x.size(0), 1), Config.BOS_IDX, device=x.device)\n",
        "            dec_hidden, dec_cell = hidden, cell\n",
        "            preds = []\n",
        "\n",
        "            for _ in range(50):\n",
        "                embedded_input = self.tgt_embedding(dec_input)\n",
        "                output, (dec_hidden, dec_cell) = self.decoder(embedded_input, (dec_hidden, dec_cell))\n",
        "                top1 = self.output_head(output.squeeze(1)).argmax(1).unsqueeze(1)\n",
        "                preds.append(top1)\n",
        "                dec_input = top1\n",
        "                if (top1 == Config.EOS_IDX).all(): break\n",
        "            return torch.cat(preds, dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 2. BAHDANAU (ADDITIVE ATTENTION)\n",
        "# ==========================================\n",
        "class BahdanauSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.src_embedding = nn.Embedding(vocab.total_tgt_tokens, Config.D_MODEL, padding_idx=Config.PAD_IDX)\n",
        "        self.tgt_embedding = nn.Embedding(vocab.total_tgt_tokens, 2*Config.D_MODEL, padding_idx=Config.PAD_IDX)\n",
        "\n",
        "        self.encoder = nn.LSTM(Config.D_MODEL, Config.D_MODEL, Config.N_ENC_LAYERS, batch_first=True, dropout=Config.DROPOUT, bidirectional=True)\n",
        "        self.decoder = nn.LSTM(4*Config.D_MODEL, 2*Config.D_MODEL, Config.N_DEC_LAYERS, batch_first=True, dropout=Config.DROPOUT)\n",
        "\n",
        "        self.attn = nn.Linear(4*Config.D_MODEL, 2*Config.D_MODEL)\n",
        "        self.v = nn.Linear(2*Config.D_MODEL, 1, bias=False)\n",
        "        self.output_head = nn.Linear(2*Config.D_MODEL, vocab.total_tgt_tokens)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        embedded_x = self.src_embedding(x)\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embedded_x)\n",
        "\n",
        "        hidden = hidden.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "        cell = cell.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "\n",
        "        dec_input = y[:, 0].unsqueeze(1)\n",
        "        dec_hidden, dec_cell = hidden, cell\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(1, y.shape[1]):\n",
        "            # Attention\n",
        "            last_hidden = dec_hidden[-1].unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
        "            energy = torch.tanh(self.attn(torch.cat((last_hidden, encoder_outputs), dim=2)))\n",
        "            attention = F.softmax(self.v(energy).squeeze(2), dim=1).unsqueeze(1)\n",
        "            context = torch.bmm(attention, encoder_outputs)\n",
        "\n",
        "            embedded_input = self.tgt_embedding(dec_input)\n",
        "            rnn_input = torch.cat((embedded_input, context), dim=2)\n",
        "            output, (dec_hidden, dec_cell) = self.decoder(rnn_input, (dec_hidden, dec_cell))\n",
        "\n",
        "            prediction = self.output_head(output.squeeze(1))\n",
        "            outputs.append(prediction.unsqueeze(1))\n",
        "            dec_input = y[:, t].unsqueeze(1)\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "    def predict(self, x):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            embedded_x = self.src_embedding(x)\n",
        "            encoder_outputs, (hidden, cell) = self.encoder(embedded_x)\n",
        "\n",
        "            hidden = hidden.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "            cell = cell.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "\n",
        "            dec_input = torch.full((x.size(0), 1), Config.BOS_IDX, device=x.device)\n",
        "            dec_hidden, dec_cell = hidden, cell\n",
        "            preds = []\n",
        "\n",
        "            for _ in range(50):\n",
        "                last_hidden = dec_hidden[-1].unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
        "                energy = torch.tanh(self.attn(torch.cat((last_hidden, encoder_outputs), dim=2)))\n",
        "                attention = F.softmax(self.v(energy).squeeze(2), dim=1).unsqueeze(1)\n",
        "                context = torch.bmm(attention, encoder_outputs)\n",
        "\n",
        "                embedded_input = self.tgt_embedding(dec_input)\n",
        "                rnn_input = torch.cat((embedded_input, context), dim=2)\n",
        "                output, (dec_hidden, dec_cell) = self.decoder(rnn_input, (dec_hidden, dec_cell))\n",
        "\n",
        "                top1 = self.output_head(output.squeeze(1)).argmax(1).unsqueeze(1)\n",
        "                preds.append(top1)\n",
        "                dec_input = top1\n",
        "                if (top1 == Config.EOS_IDX).all(): break\n",
        "\n",
        "            return torch.cat(preds, dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 3. LUONG (DOT ATTENTION)\n",
        "# ==========================================\n",
        "class LuongSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.src_embedding = nn.Embedding(vocab.total_tgt_tokens, Config.D_MODEL, padding_idx=Config.PAD_IDX)\n",
        "        self.tgt_embedding = nn.Embedding(vocab.total_tgt_tokens, 2*Config.D_MODEL, padding_idx=Config.PAD_IDX)\n",
        "        self.encoder = nn.LSTM(Config.D_MODEL, Config.D_MODEL, Config.N_ENC_LAYERS, batch_first=True, dropout=Config.DROPOUT, bidirectional=True)\n",
        "        self.decoder = nn.LSTM(2*Config.D_MODEL, 2*Config.D_MODEL, Config.N_DEC_LAYERS, batch_first=True, dropout=Config.DROPOUT)\n",
        "\n",
        "        self.concat = nn.Linear(4*Config.D_MODEL, 2*Config.D_MODEL)\n",
        "        self.output_head = nn.Linear(2*Config.D_MODEL, vocab.total_tgt_tokens)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        embedded_x = self.src_embedding(x)\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embedded_x)\n",
        "\n",
        "        hidden = hidden.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "        cell = cell.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "\n",
        "        dec_input = y[:, 0].unsqueeze(1)\n",
        "        dec_hidden, dec_cell = hidden, cell\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(1, y.shape[1]):\n",
        "            embedded_input = self.tgt_embedding(dec_input)\n",
        "            rnn_output, (dec_hidden, dec_cell) = self.decoder(embedded_input, (dec_hidden, dec_cell))\n",
        "\n",
        "            # Dot Attention\n",
        "            scores = torch.bmm(rnn_output, encoder_outputs.permute(0, 2, 1))\n",
        "            attn_weights = F.softmax(scores, dim=2)\n",
        "            context = torch.bmm(attn_weights, encoder_outputs)\n",
        "\n",
        "            concat_input = torch.cat((context, rnn_output), dim=2)\n",
        "            h_tilde = torch.tanh(self.concat(concat_input))\n",
        "\n",
        "            prediction = self.output_head(h_tilde.squeeze(1))\n",
        "            outputs.append(prediction.unsqueeze(1))\n",
        "            dec_input = y[:, t].unsqueeze(1)\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "    def predict(self, x):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            embedded_x = self.src_embedding(x)\n",
        "            encoder_outputs, (hidden, cell) = self.encoder(embedded_x)\n",
        "\n",
        "            hidden = hidden.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "            cell = cell.view(Config.N_ENC_LAYERS, 2, x.size(0), -1).permute(0, 2, 1, 3).reshape(Config.N_ENC_LAYERS, x.size(0), -1)\n",
        "\n",
        "            dec_input = torch.full((x.size(0), 1), Config.BOS_IDX, device=x.device)\n",
        "            dec_hidden, dec_cell = hidden, cell\n",
        "            preds = []\n",
        "\n",
        "            for _ in range(50):\n",
        "                embedded_input = self.tgt_embedding(dec_input)\n",
        "                rnn_output, (dec_hidden, dec_cell) = self.decoder(embedded_input, (dec_hidden, dec_cell))\n",
        "\n",
        "                scores = torch.bmm(rnn_output, encoder_outputs.permute(0, 2, 1))\n",
        "                attn_weights = F.softmax(scores, dim=2)\n",
        "                context = torch.bmm(attn_weights, encoder_outputs)\n",
        "\n",
        "                concat_input = torch.cat((context, rnn_output), dim=2)\n",
        "                h_tilde = torch.tanh(self.concat(concat_input))\n",
        "\n",
        "                top1 = self.output_head(h_tilde.squeeze(1)).argmax(1).unsqueeze(1)\n",
        "                preds.append(top1)\n",
        "                dec_input = top1\n",
        "                if (top1 == Config.EOS_IDX).all(): break\n",
        "\n",
        "            return torch.cat(preds, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADMh2R1sAvHp",
        "outputId": "7865dda1-5c3a-4c68-ea14-0822b082194d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile trainer.py\n",
        "import torch\n",
        "from config import Config\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "\n",
        "def train_epoch(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, tgt in iterator:\n",
        "        src, tgt = src.to(Config.DEVICE), tgt.to(Config.DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        # Output: [bs, trg_len, vocab_size], Tgt: [bs, trg_len]\n",
        "        # Flatten ƒë·ªÉ t√≠nh loss, b·ªè qua c·ªôt ƒë·∫ßu ti√™n c·ªßa output (t∆∞∆°ng ·ª©ng v·ªõi d·ª± ƒëo√°n t·ª´ ƒë·∫ßu v√†o <bos>)\n",
        "        output_dim = output.shape[-1]\n",
        "        loss = criterion(output.reshape(-1, output_dim), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), Config.CLIP)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def calculate_rouge(model, iterator, vocab):\n",
        "    model.eval()\n",
        "    rouge = ROUGEScore()\n",
        "    preds_text = []\n",
        "    targets_text = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in iterator:\n",
        "            src = src.to(Config.DEVICE)\n",
        "            # G·ªçi h√†m predict (l∆∞u √Ω b·∫°n c·∫ßn implement predict trong models.py)\n",
        "            # ·ªû ƒë√¢y d√πng logic gi·∫£ ƒë·ªãnh n·∫øu ch∆∞a implement\n",
        "            pred_indices = model.predict(src)\n",
        "            if pred_indices is None: continue\n",
        "\n",
        "            for i in range(src.size(0)):\n",
        "                p = vocab.decode(pred_indices[i].tolist())\n",
        "                t = vocab.decode(tgt[i].tolist())\n",
        "                preds_text.append(p)\n",
        "                targets_text.append(t)\n",
        "\n",
        "    if not preds_text: return 0.0\n",
        "    return rouge(preds_text, targets_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfcPc0-cA7dn",
        "outputId": "c3533024-d65c-448d-e9f7-eb912466b8fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.py\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from config import Config\n",
        "from vocab import Vocab\n",
        "\n",
        "class PhoMTDataset(Dataset):\n",
        "    def __init__(self, json_path, vocab, src_key, tgt_key):\n",
        "        self.vocab = vocab\n",
        "        self.data = []\n",
        "\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            raw_data = json.load(f)\n",
        "\n",
        "        for item in raw_data:\n",
        "            # L∆∞u l·∫°i token d·∫°ng s·ªë lu√¥n ƒë·ªÉ train cho nhanh\n",
        "            src_text = item[src_key]\n",
        "            tgt_text = item[tgt_key]\n",
        "\n",
        "            src_indices = self.vocab.encode(src_text)\n",
        "            tgt_indices = self.vocab.encode(tgt_text)\n",
        "\n",
        "            self.data.append((torch.tensor(src_indices), torch.tensor(tgt_indices)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_pad = pad_sequence(src_batch, padding_value=Config.PAD_IDX, batch_first=True)\n",
        "    tgt_pad = pad_sequence(tgt_batch, padding_value=Config.PAD_IDX, batch_first=True)\n",
        "    return src_pad, tgt_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys5EQXzDAbRO",
        "outputId": "4a4a9152-0ce4-49af-b103-b2d659ee6752"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSMxbXsweDoR",
        "outputId": "3b91412f-ef5d-4976-d9ed-8f0f80301e91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Import modules\n",
        "from config import Config\n",
        "from vocab import Vocab\n",
        "from dataset import PhoMTDataset, collate_fn\n",
        "from models import VanillaSeq2Seq, BahdanauSeq2Seq, LuongSeq2Seq\n",
        "from trainer import train_epoch, calculate_rouge\n",
        "\n",
        "# 1. SETUP & LOAD DATA\n",
        "print(\"üöÄ ƒêang kh·ªüi t·∫°o d·ªØ li·ªáu...\")\n",
        "vocab = Vocab()\n",
        "# X√¢y d·ª±ng vocab t·ª´ t·∫≠p train (Gi·∫£ s·ª≠ file t·ªìn t·∫°i)\n",
        "vocab.build_vocab(Config.TRAIN_PATH, Config.SRC_KEY, Config.TGT_KEY)\n",
        "\n",
        "train_ds = PhoMTDataset(Config.TRAIN_PATH, vocab, Config.SRC_KEY, Config.TGT_KEY)\n",
        "test_ds = PhoMTDataset(Config.TEST_PATH, vocab, Config.SRC_KEY, Config.TGT_KEY)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn) # BS=1 for inference\n",
        "\n",
        "print(f\"‚úÖ Data Loaded. Train size: {len(train_ds)}, Test size: {len(test_ds)}\")\n",
        "\n",
        "# 2. LIST C√ÅC MODEL C·∫¶N TRAIN\n",
        "model_classes = [\n",
        "    (\"Vanilla Seq2Seq\", VanillaSeq2Seq),\n",
        "    (\"Bahdanau Attention\", BahdanauSeq2Seq),\n",
        "    (\"Luong Attention\", LuongSeq2Seq)\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "# 3. TRAINING LOOP\n",
        "for name, ModelClass in model_classes:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üîÑ ƒêang train model: {name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Init Model\n",
        "    model = ModelClass(vocab).to(Config.DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=Config.PAD_IDX)\n",
        "\n",
        "    # Train\n",
        "    model.train()\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"   Epoch {epoch+1}/{Config.NUM_EPOCHS} | Loss: {loss:.4f}\")\n",
        "\n",
        "    # Evaluate ROUGE\n",
        "    print(f\"üìä ƒêang t√≠nh ROUGE-L cho {name}...\")\n",
        "    try:\n",
        "        rouge_score = calculate_rouge(model, test_loader, vocab)\n",
        "        # L·∫•y Rouge-L Fmeasure\n",
        "        r_l = rouge_score['rougeL_fmeasure'].item()\n",
        "        print(f\"   üî• ROUGE-L F1: {r_l:.4f}\")\n",
        "        results.append({\"Model\": name, \"Rouge-L\": r_l, \"Loss\": loss})\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói t√≠nh Rouge: {e}\")\n",
        "        results.append({\"Model\": name, \"Rouge-L\": 0.0, \"Loss\": loss})\n",
        "\n",
        "# 4. SHOW COMPARISON TABLE\n",
        "print(\"\\n\" + \"#\"*50)\n",
        "print(\"üèÜ K·∫æT QU·∫¢ SO S√ÅNH\")\n",
        "print(\"#\"*50)\n",
        "df = pd.DataFrame(results)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0LKFl3teHNR",
        "outputId": "3518036e-5092-4bff-bb67-63afaa96760b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ ƒêang kh·ªüi t·∫°o d·ªØ li·ªáu...\n",
            "ƒêang x√¢y d·ª±ng Vocab t·ª´ /content/small-train.json...\n",
            "Vocab size: 13190\n",
            "‚úÖ Data Loaded. Train size: 20000, Test size: 2000\n",
            "\n",
            "==================================================\n",
            "üîÑ ƒêang train model: Vanilla Seq2Seq\n",
            "==================================================\n",
            "   Epoch 5/15 | Loss: 4.3683\n",
            "   Epoch 10/15 | Loss: 3.6961\n",
            "   Epoch 15/15 | Loss: 3.2994\n",
            "üìä ƒêang t√≠nh ROUGE-L cho Vanilla Seq2Seq...\n",
            "   üî• ROUGE-L F1: 0.3450\n",
            "\n",
            "==================================================\n",
            "üîÑ ƒêang train model: Bahdanau Attention\n",
            "==================================================\n",
            "   Epoch 5/15 | Loss: 4.4078\n",
            "   Epoch 10/15 | Loss: 3.6943\n",
            "   Epoch 15/15 | Loss: 3.2833\n",
            "üìä ƒêang t√≠nh ROUGE-L cho Bahdanau Attention...\n",
            "   üî• ROUGE-L F1: 0.3486\n",
            "\n",
            "==================================================\n",
            "üîÑ ƒêang train model: Luong Attention\n",
            "==================================================\n",
            "   Epoch 5/15 | Loss: 4.1112\n",
            "   Epoch 10/15 | Loss: 2.9502\n",
            "   Epoch 15/15 | Loss: 2.3587\n",
            "üìä ƒêang t√≠nh ROUGE-L cho Luong Attention...\n",
            "   üî• ROUGE-L F1: 0.4576\n",
            "\n",
            "##################################################\n",
            "üèÜ K·∫æT QU·∫¢ SO S√ÅNH\n",
            "##################################################\n",
            "                Model   Rouge-L      Loss\n",
            "0     Vanilla Seq2Seq  0.345024  3.299417\n",
            "1  Bahdanau Attention  0.348576  3.283308\n",
            "2     Luong Attention  0.457616  2.358734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atOO7zg2giLt"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}