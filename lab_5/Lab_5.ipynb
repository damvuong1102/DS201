{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# ==========================================\n",
        "# 1. Cấu hình Hyperparameters\n",
        "# ==========================================\n",
        "Config = {\n",
        "    'vocab_size': 10000,\n",
        "    'd_model': 128,\n",
        "    'n_head': 4,\n",
        "    'n_layers': 3,\n",
        "    'd_ff': 512,\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'max_len': 100\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Xử lý dữ liệu (Dataset & Tokenizer)\n",
        "# ==========================================\n",
        "\n",
        "class TextTokenizer:\n",
        "    def __init__(self, vocab_size=10000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "        self.idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
        "\n",
        "    def fit(self, texts):\n",
        "        # Tách từ đơn giản bằng khoảng trắng\n",
        "        all_words = []\n",
        "        for text in texts:\n",
        "            all_words.extend(str(text).lower().split())\n",
        "\n",
        "        counts = Counter(all_words)\n",
        "        # Lấy top từ phổ biến nhất\n",
        "        common_words = counts.most_common(self.vocab_size - 2)\n",
        "\n",
        "        for word, _ in common_words:\n",
        "            if word not in self.word2idx:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "    def encode(self, text, max_len=100):\n",
        "        words = str(text).lower().split()\n",
        "        indices = [self.word2idx.get(w, 1) for w in words] # 1 là <UNK>\n",
        "\n",
        "        # Padding hoặc cắt ngắn (Truncate)\n",
        "        if len(indices) < max_len:\n",
        "            indices += [0] * (max_len - len(indices)) # 0 là <PAD>\n",
        "        else:\n",
        "            indices = indices[:max_len]\n",
        "        return indices\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ViOCDDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_len=100, is_train=False):\n",
        "        # 1. Đọc file JSON\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            raw_data = json.load(f)\n",
        "\n",
        "        # Chuyển đổi từ dict {\"0\": {...}, \"1\": {...}} sang list [{...}, {...}]\n",
        "        data_list = list(raw_data.values())\n",
        "        self.df = pd.DataFrame(data_list)\n",
        "\n",
        "        self.text_col = 'review'\n",
        "        self.label_col = 'domain'\n",
        "\n",
        "        self.texts = self.df[self.text_col].values\n",
        "        self.labels = self.df[self.label_col].values\n",
        "\n",
        "        if is_train:\n",
        "            tokenizer.fit(self.texts)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        unique_labels = sorted(list(set(self.labels)))\n",
        "        self.label_map = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
        "        self.num_classes = len(unique_labels)\n",
        "\n",
        "        print(f\"Loaded {len(self.texts)} samples from {file_path}\")\n",
        "        print(f\"Domains found: {self.label_map}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label_str = self.labels[idx]\n",
        "\n",
        "        # Encode text thành các con số\n",
        "        encoded_text = self.tokenizer.encode(text, self.max_len)\n",
        "\n",
        "        # Chuyển label từ chữ (ví dụ 'mobile') sang số (ví dụ 0)\n",
        "        label_idx = self.label_map[label_str]\n",
        "\n",
        "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label_idx, dtype=torch.long)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Xây dựng Mô hình (Transformer Encoder)\n",
        "# ==========================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Tạo ma trận positional encoding\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, d_model]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, config, num_classes):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        # 1. Embedding Layer\n",
        "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        # 2. Positional Encoding\n",
        "        self.pos_encoder = PositionalEncoding(config['d_model'], config['max_len'])\n",
        "\n",
        "        # 3. Transformer Encoder Layers (Stacked)\n",
        "        # encoder_layer định nghĩa 1 block (MultiHeadAttn + FeedForward)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config['d_model'],\n",
        "            nhead=config['n_head'],\n",
        "            dim_feedforward=config['d_ff'],\n",
        "            dropout=config['dropout'],\n",
        "            batch_first=True # Quan trọng: input shape là [batch, seq, feature]\n",
        "        )\n",
        "\n",
        "        # Stack 3 lớp encoder\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config['n_layers'])\n",
        "\n",
        "        # 4. Classification Head\n",
        "        self.fc = nn.Linear(config['d_model'], num_classes)\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, max_len]\n",
        "\n",
        "        # Mask padding (để attention không chú ý vào token 0 <PAD>)\n",
        "        # src_key_padding_mask: True ở vị trí padding\n",
        "        src_key_padding_mask = (x == 0)\n",
        "\n",
        "        # Embedding & Positional Encoding\n",
        "        x = self.embedding(x) * math.sqrt(Config['d_model'])\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Qua Transformer Encoder\n",
        "        # Output: [batch_size, max_len, d_model]\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Pooling: Lấy vector trung bình của các từ (Global Average Pooling)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "# ==========================================\n",
        "# 4. Huấn luyện và Đánh giá\n",
        "# ==========================================\n",
        "\n",
        "def train_model():\n",
        "    # Load data\n",
        "    tokenizer = TextTokenizer(vocab_size=Config['vocab_size'])\n",
        "\n",
        "    print(\"Loading Train set...\")\n",
        "    train_dataset = ViOCDDataset('train.json', tokenizer, Config['max_len'], is_train=True)\n",
        "    print(\"Loading Dev set...\")\n",
        "    dev_dataset = ViOCDDataset('dev.json', tokenizer, Config['max_len'])\n",
        "    print(\"Loading Test set...\")\n",
        "    test_dataset = ViOCDDataset('test.json', tokenizer, Config['max_len'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config['batch_size'], shuffle=True)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=Config['batch_size'])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config['batch_size'])\n",
        "\n",
        "    print(f\"Number of classes: {train_dataset.num_classes}\")\n",
        "\n",
        "    # Init model\n",
        "    model = TransformerClassifier(Config, num_classes=train_dataset.num_classes).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config['learning_rate'])\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(Config['epochs']):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in dev_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "                outputs = model(texts)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{Config['epochs']}, Loss: {total_loss/len(train_loader):.4f}, Dev Acc: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    # Testing\n",
        "    print(\"\\nEvaluating on Test set...\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in test_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Go!\")\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYvWmBJ86dP6",
        "outputId": "792d3cd8-0177-4573-e885-2df289b3f59f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Go!\n",
            "Loading Train set...\n",
            "Loaded 4387 samples from train.json\n",
            "Domains found: {'app': 0, 'cosmetic': 1, 'fashion': 2, 'mobile': 3}\n",
            "Loading Dev set...\n",
            "Loaded 548 samples from dev.json\n",
            "Domains found: {'app': 0, 'cosmetic': 1, 'fashion': 2, 'mobile': 3}\n",
            "Loading Test set...\n",
            "Loaded 549 samples from test.json\n",
            "Domains found: {'app': 0, 'cosmetic': 1, 'fashion': 2, 'mobile': 3}\n",
            "Number of classes: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6890, Dev Acc: 70.26%\n",
            "Epoch 2/10, Loss: 0.3770, Dev Acc: 81.57%\n",
            "Epoch 3/10, Loss: 0.2957, Dev Acc: 79.74%\n",
            "Epoch 4/10, Loss: 0.2611, Dev Acc: 80.47%\n",
            "Epoch 5/10, Loss: 0.2338, Dev Acc: 80.47%\n",
            "Epoch 6/10, Loss: 0.2537, Dev Acc: 83.21%\n",
            "Epoch 7/10, Loss: 0.1838, Dev Acc: 82.66%\n",
            "Epoch 8/10, Loss: 0.1792, Dev Acc: 81.02%\n",
            "Epoch 9/10, Loss: 0.1627, Dev Acc: 85.58%\n",
            "Epoch 10/10, Loss: 0.1440, Dev Acc: 86.31%\n",
            "\n",
            "Evaluating on Test set...\n",
            "Test Accuracy: 86.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# ==========================================\n",
        "# 1. Cấu hình Hyperparameters\n",
        "# ==========================================\n",
        "Config = {\n",
        "    'vocab_size': 10000,\n",
        "    'd_model': 128,\n",
        "    'n_head': 4,\n",
        "    'n_layers': 3,\n",
        "    'd_ff': 512,\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 10,\n",
        "    'max_len': 100,\n",
        "    'pad_idx': 0,\n",
        "    'unk_idx': 1,\n",
        "    'ignore_index': -100\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. Xử lý dữ liệu (Vocab & Dataset)\n",
        "# ==========================================\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, vocab_size=10000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word2idx = {\"<PAD>\": Config['pad_idx'], \"<UNK>\": Config['unk_idx']}\n",
        "        self.idx2word = {Config['pad_idx']: \"<PAD>\", Config['unk_idx']: \"<UNK>\"}\n",
        "        self.tag2idx = {}\n",
        "        self.idx2tag = {}\n",
        "\n",
        "    def build_vocab(self, words_list, tags_list):\n",
        "        # 1. Build Word Vocab\n",
        "        all_words = [w.lower() for sent in words_list for w in sent]\n",
        "        counts = Counter(all_words)\n",
        "        common_words = counts.most_common(self.vocab_size - 2)\n",
        "\n",
        "        for word, _ in common_words:\n",
        "            if word not in self.word2idx:\n",
        "                idx = len(self.word2idx)\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "        # 2. Build Tag Vocab\n",
        "        # Thêm padding tag để map đồng bộ, dù loss function sẽ ignore nó\n",
        "        all_tags = sorted(list(set([t for sent in tags_list for t in sent])))\n",
        "        self.tag2idx = {tag: i for i, tag in enumerate(all_tags)}\n",
        "        self.idx2tag = {i: tag for tag, i in self.tag2idx.items()}\n",
        "\n",
        "        print(f\"Vocab size: {len(self.word2idx)}\")\n",
        "        print(f\"Num tags: {len(self.tag2idx)}\")\n",
        "        print(f\"Tags map: {self.tag2idx}\")\n",
        "\n",
        "    def encode_text(self, sent_list, max_len=100):\n",
        "        # Input là list các từ: [\"Tôi\", \"đi\", \"học\"]\n",
        "        indices = [self.word2idx.get(w.lower(), Config['unk_idx']) for w in sent_list]\n",
        "\n",
        "        if len(indices) < max_len:\n",
        "            indices += [Config['pad_idx']] * (max_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:max_len]\n",
        "        return indices\n",
        "\n",
        "    def encode_tags(self, tag_list, max_len=100):\n",
        "        # Input là list các nhãn: [\"O\", \"O\", \"O\"]\n",
        "        indices = [self.tag2idx.get(t, 0) for t in tag_list]\n",
        "\n",
        "        # Padding cho nhãn: Dùng ignore_index (-100) để model không tính loss vào đây\n",
        "        if len(indices) < max_len:\n",
        "            indices += [Config['ignore_index']] * (max_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:max_len]\n",
        "        return indices\n",
        "\n",
        "class PhoNERDataset(Dataset):\n",
        "    def __init__(self, file_path, vocab, max_len=100, is_train=False):\n",
        "        self.sentences = []\n",
        "        self.tags = []\n",
        "\n",
        "        print(f\"Processing {file_path}...\")\n",
        "\n",
        "        try:\n",
        "            # Cấu trúc: {\"words\": [[s1], [s2]], \"tags\": [[t1], [t2]]}\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                self.sentences = data['words']\n",
        "                self.tags = data['tags']\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # Xử lý trường hợp file là dạng JSON Lines hoặc bị nối đuôi nhau\n",
        "            print(f\"-> Detected JSON Lines format or multiple objects in {file_path}. Switching parse mode.\")\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                # Đọc từng dòng\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line: continue\n",
        "                    try:\n",
        "                        obj = json.loads(line)\n",
        "                        # Kiểm tra xem dòng này chứa 1 list các câu hay chỉ 1 câu\n",
        "                        if 'words' in obj and 'tags' in obj:\n",
        "                            # Nếu words là list lồng nhau [[...]], dùng extend\n",
        "                            if len(obj['words']) > 0 and isinstance(obj['words'][0], list):\n",
        "                                self.sentences.extend(obj['words'])\n",
        "                                self.tags.extend(obj['tags'])\n",
        "                            else:\n",
        "                                # Nếu words là list đơn [...], dùng append\n",
        "                                self.sentences.append(obj['words'])\n",
        "                                self.tags.append(obj['tags'])\n",
        "                    except json.JSONDecodeError:\n",
        "                        continue\n",
        "\n",
        "        # Validation: Kiểm tra xem dữ liệu có khớp không\n",
        "        assert len(self.sentences) == len(self.tags), \"Mismatch between sentences and tags length!\"\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Nếu là train set thì build vocab\n",
        "        if is_train:\n",
        "            self.vocab.build_vocab(self.sentences, self.tags)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag = self.tags[idx]\n",
        "\n",
        "        encoded_sent = self.vocab.encode_text(sent, self.max_len)\n",
        "        encoded_tag = self.vocab.encode_tags(tag, self.max_len)\n",
        "\n",
        "        return torch.tensor(encoded_sent, dtype=torch.long), torch.tensor(encoded_tag, dtype=torch.long)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Xây dựng Mô hình (Token Classification)\n",
        "# ==========================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class TransformerTagger(nn.Module):\n",
        "    def __init__(self, config, num_tags):\n",
        "        super(TransformerTagger, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(len(config['vocab'].word2idx), config['d_model'], padding_idx=config['pad_idx'])\n",
        "        self.pos_encoder = PositionalEncoding(config['d_model'], config['max_len'])\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config['d_model'],\n",
        "            nhead=config['n_head'],\n",
        "            dim_feedforward=config['d_ff'],\n",
        "            dropout=config['dropout'],\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config['n_layers'])\n",
        "\n",
        "        # Output layer: Project từ d_model ra số lượng tags\n",
        "        # Shape output sẽ là [batch, seq_len, num_tags]\n",
        "        self.fc = nn.Linear(config['d_model'], num_tags)\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len]\n",
        "\n",
        "        # Tạo mask để Transformer không attention vào padding\n",
        "        src_key_padding_mask = (x == Config['pad_idx'])\n",
        "\n",
        "        x = self.embedding(x) * math.sqrt(Config['d_model'])\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Output Transformer: [batch, seq_len, d_model]\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Ta cần dự đoán cho từng vị trí token.\n",
        "        logits = self.fc(x) # [batch, seq_len, num_tags]\n",
        "\n",
        "        return logits\n",
        "\n",
        "# ==========================================\n",
        "# 4. Huấn luyện và Đánh giá\n",
        "# ==========================================\n",
        "\n",
        "def calculate_accuracy(preds, y, ignore_index):\n",
        "    # Flatten: [batch * seq_len]\n",
        "    preds = preds.view(-1)\n",
        "    y = y.view(-1)\n",
        "\n",
        "    # Chỉ tính accuracy trên các token thật (bỏ qua padding -100)\n",
        "    mask = (y != ignore_index)\n",
        "    correct = (preds[mask] == y[mask]).sum()\n",
        "    total = mask.sum()\n",
        "\n",
        "    return correct.item(), total.item()\n",
        "\n",
        "def train_model():\n",
        "    vocab = Vocab(vocab_size=Config['vocab_size'])\n",
        "    Config['vocab'] = vocab # Lưu vocab vào config để model dùng embedding size chuẩn\n",
        "\n",
        "    print(\"--- LOADING DATA ---\")\n",
        "    train_dataset = PhoNERDataset('train_word.json', vocab, Config['max_len'], is_train=True)\n",
        "\n",
        "    # Dev/Test dùng chung vocab của Train\n",
        "    dev_dataset = PhoNERDataset('dev_word.json', vocab, Config['max_len'])\n",
        "    test_dataset = PhoNERDataset('test_word.json', vocab, Config['max_len'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config['batch_size'], shuffle=True)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=Config['batch_size'])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config['batch_size'])\n",
        "\n",
        "    num_tags = len(vocab.tag2idx)\n",
        "\n",
        "    # Init Model\n",
        "    model = TransformerTagger(Config, num_tags=num_tags).to(device)\n",
        "\n",
        "    # Loss function: ignore_index=-100 để bỏ qua padding khi tính loss\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=Config['ignore_index'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config['learning_rate'])\n",
        "\n",
        "    print(\"--- START TRAINING ---\")\n",
        "    for epoch in range(Config['epochs']):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for texts, tags in train_loader:\n",
        "            texts, tags = texts.to(device), tags.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts) # [batch, seq_len, num_tags]\n",
        "\n",
        "            # Reshape để tính loss\n",
        "            # outputs: [batch * seq_len, num_tags]\n",
        "            # tags: [batch * seq_len]\n",
        "            loss = criterion(outputs.view(-1, num_tags), tags.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calc Accuracy\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "            c, t = calculate_accuracy(predicted, tags, Config['ignore_index'])\n",
        "            total_correct += c\n",
        "            total_tokens += t\n",
        "\n",
        "        train_acc = 100 * total_correct / total_tokens\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_tokens = 0\n",
        "        with torch.no_grad():\n",
        "            for texts, tags in dev_loader:\n",
        "                texts, tags = texts.to(device), tags.to(device)\n",
        "                outputs = model(texts)\n",
        "                _, predicted = torch.max(outputs, 2)\n",
        "\n",
        "                c, t = calculate_accuracy(predicted, tags, Config['ignore_index'])\n",
        "                val_correct += c\n",
        "                val_tokens += t\n",
        "\n",
        "        val_acc = 100 * val_correct / val_tokens\n",
        "        print(f\"Epoch {epoch+1}/{Config['epochs']} | Loss: {total_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}% | Dev Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Testing\n",
        "    print(\"\\n--- EVALUATING ON TEST SET ---\")\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, tags in test_loader:\n",
        "            texts, tags = texts.to(device), tags.to(device)\n",
        "            outputs = model(texts)\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "            c, t = calculate_accuracy(predicted, tags, Config['ignore_index'])\n",
        "            test_correct += c\n",
        "            test_tokens += t\n",
        "\n",
        "    print(f\"Test Accuracy: {100 * test_correct / test_tokens:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDVuf_DS8AaB",
        "outputId": "6cbc94b3-bef5-4019-a556-907ad241f96e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "--- LOADING DATA ---\n",
            "Processing train_word.json...\n",
            "-> Detected JSON Lines format or multiple objects in train_word.json. Switching parse mode.\n",
            "Vocab size: 4741\n",
            "Num tags: 20\n",
            "Tags map: {'B-AGE': 0, 'B-DATE': 1, 'B-GENDER': 2, 'B-JOB': 3, 'B-LOCATION': 4, 'B-NAME': 5, 'B-ORGANIZATION': 6, 'B-PATIENT_ID': 7, 'B-SYMPTOM_AND_DISEASE': 8, 'B-TRANSPORTATION': 9, 'I-AGE': 10, 'I-DATE': 11, 'I-JOB': 12, 'I-LOCATION': 13, 'I-NAME': 14, 'I-ORGANIZATION': 15, 'I-PATIENT_ID': 16, 'I-SYMPTOM_AND_DISEASE': 17, 'I-TRANSPORTATION': 18, 'O': 19}\n",
            "Processing dev_word.json...\n",
            "-> Detected JSON Lines format or multiple objects in dev_word.json. Switching parse mode.\n",
            "Processing test_word.json...\n",
            "-> Detected JSON Lines format or multiple objects in test_word.json. Switching parse mode.\n",
            "--- START TRAINING ---\n",
            "Epoch 1/10 | Loss: 0.6772 | Train Acc: 83.10% | Dev Acc: 84.92%\n",
            "Epoch 2/10 | Loss: 0.3235 | Train Acc: 89.91% | Dev Acc: 88.00%\n",
            "Epoch 3/10 | Loss: 0.2415 | Train Acc: 92.28% | Dev Acc: 89.42%\n",
            "Epoch 4/10 | Loss: 0.2063 | Train Acc: 93.30% | Dev Acc: 89.98%\n",
            "Epoch 5/10 | Loss: 0.1782 | Train Acc: 93.97% | Dev Acc: 90.10%\n",
            "Epoch 6/10 | Loss: 0.1556 | Train Acc: 94.74% | Dev Acc: 90.55%\n",
            "Epoch 7/10 | Loss: 0.1430 | Train Acc: 95.07% | Dev Acc: 91.09%\n",
            "Epoch 8/10 | Loss: 0.1280 | Train Acc: 95.51% | Dev Acc: 91.08%\n",
            "Epoch 9/10 | Loss: 0.1207 | Train Acc: 95.85% | Dev Acc: 90.67%\n",
            "Epoch 10/10 | Loss: 0.1166 | Train Acc: 95.89% | Dev Acc: 91.07%\n",
            "\n",
            "--- EVALUATING ON TEST SET ---\n",
            "Test Accuracy: 90.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hf8os75OFLaC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}